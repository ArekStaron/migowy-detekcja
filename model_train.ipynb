{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3589b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ab69d",
   "metadata": {},
   "source": [
    "robienie dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6415f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data sety są z innych języków migowych \n",
    "\"\"\"\n",
    "# merging data\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "dataset_dir = \"Sign-Language-Digits-Dataset-master\\Dataset\"\n",
    "dataset_dir_2 = \"data_2\"\n",
    "\n",
    "dataset_dirs = [dataset_dir , dataset_dir_2]\n",
    "\n",
    "out_dir = Path(\"merged_dataset\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for d in dataset_dirs:\n",
    "    for cls in os.listdir(d):\n",
    "        src = Path(d) / cls\n",
    "        if not src.is_dir(): continue\n",
    "        dst = out_dir / cls\n",
    "        dst.mkdir(exist_ok=True)\n",
    "        for f in os.listdir(src):\n",
    "            shutil.copy(src / f, dst / f)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a4dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 21\n",
      "2000 52\n",
      "3000 95\n",
      "4000 98\n",
      "5000 102\n",
      "6000 111\n",
      "7000 111\n",
      "8000 111\n",
      "9000 137\n",
      "10000 145\n",
      "11000 152\n",
      "12000 166\n",
      "13000 166\n",
      "14000 166\n",
      "15000 166\n",
      "16000 166\n",
      "17000 166\n",
      "18000 166\n",
      "19000 166\n",
      "20000 166\n",
      "21000 166\n",
      "22000 166\n",
      "23000 166\n",
      "24000 166\n",
      "25000 166\n",
      "26000 166\n",
      "27000 166\n",
      "28000 166\n",
      "29000 166\n",
      "30000 168\n",
      "31000 187\n",
      "32000 432\n",
      "33000 524\n",
      "34000 534\n",
      "35000 552\n",
      "36000 577\n",
      "37000 577\n",
      "38000 578\n",
      "39000 578\n",
      "40000 578\n",
      "41000 578\n",
      "42000 578\n",
      "43000 578\n",
      "44000 578\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mp_hands =  mp.solutions.hands\n",
    "hands_model  = mp_hands.Hands(static_image_mode=True, max_num_hands=1 , min_detection_confidence=0.1)\n",
    "\n",
    "\n",
    "data = []\n",
    "errors = []\n",
    "\n",
    "count = 0\n",
    "merged_data_dir = \"merged_dataset\"\n",
    "for class_name in os.listdir(merged_data_dir):\n",
    "    class_dir = os.path.join(merged_data_dir , class_name)\n",
    "\n",
    "\n",
    "    for image_file in os.listdir(class_dir):\n",
    "        count +=1\n",
    "        image_path = os.path.join(class_dir , image_file)\n",
    "        image  =cv2.imread(image_path)\n",
    "        image =  cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image , (128 , 128))\n",
    "        results = hands_model.process(image)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            points = torch.tensor([[p.x , p.y , p.z] for p in hand_landmarks.landmark])\n",
    "            data.append((points ,class_name))\n",
    "        else:\n",
    "            errors.append(image_path)\n",
    "        \n",
    "        if count%1000 == 0:\n",
    "            print(count , len(errors))\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "\"\"\"torch.save(data , \"tensordata\" )   \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386372ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape obrazu: (128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"image_path = \"merged_dataset/S/0.JPG\"\n",
    "image = cv2.imread(image_path)\n",
    "print(\"Shape obrazu:\", image.shape) \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4a9b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#moj komputer nie daje rady z przetwarzaniem większej ilości niż \n",
    "\"\"\"\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "merged_data_dir = \"merged_dataset\"\n",
    "\n",
    "\n",
    "def process_image(class_name, img_file):\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands_model = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.1)    \n",
    "    \n",
    "    img_path = os.path.join(merged_data_dir, class_name, img_file)\n",
    "    \n",
    "    image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    results = hands_model.process(image)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        \n",
    "        pts = torch.tensor([[p.x, p.y, p.z] for p in results.multi_hand_landmarks[0].landmark])\n",
    "        \n",
    "        return (pts, int(class_name))\n",
    "    return None\n",
    "\n",
    "\n",
    "tasks = [(class_name, img_file) \n",
    "         \n",
    "         for class_name in os.listdir(merged_data_dir)\n",
    "         \n",
    "         for img_file in os.listdir(os.path.join(merged_data_dir, class_name))]\n",
    "\n",
    "data = []\n",
    "\n",
    "count = 0\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    \n",
    "    for res in executor.map(lambda t: process_image(*t), tasks):\n",
    "        count +=1\n",
    "        if res is not None:\n",
    "            \n",
    "            data.append(res)\n",
    "        if count % 1000 ==0:\n",
    "            print(f\"{count} ilość obrazów {len(data)} \")\n",
    "\n",
    "torch.save(data, \"hands_tensor_data_multithread.pt\")\n",
    "print(f\"Gotowe! Liczba próbek: {len(data)}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018e8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba wszystkich plików: 44062\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "total = sum(len(files) for _, _, files in os.walk(merged_data_dir))\n",
    "print(\"Liczba wszystkich plików:\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a793f1",
   "metadata": {},
   "source": [
    "szacowanie ile zajmie przejście przez dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([21, 3])\n",
      "Czas przetwarzania: 0.06308627128601074 sekund\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import time\n",
    "mp_hands = mp.solutions.hands\n",
    "hands_model = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.1)\n",
    "\n",
    "image_path = r\"merged_dataset\\0\\IMG_1128.JPG\"  \n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "start = time.time()\n",
    "\n",
    "results = hands_model.process(image)\n",
    "\n",
    "if results.multi_hand_landmarks:\n",
    "    hand_landmarks = results.multi_hand_landmarks[0]\n",
    "    points = torch.tensor([[p.x, p.y, p.z] for p in hand_landmarks.landmark])\n",
    "    print(\"Tensor shape:\", points.shape)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Czas przetwarzania:\", end - start, \"sekund\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2636fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde22f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0\n",
      "2000 0\n",
      "3000 0\n",
      "4000 0\n",
      "5000 0\n",
      "6000 0\n",
      "7000 0\n",
      "8000 0\n",
      "9000 0\n",
      "10000 0\n",
      "11000 0\n",
      "12000 0\n",
      "13000 0\n",
      "14000 0\n",
      "15000 0\n",
      "16000 0\n",
      "17000 0\n",
      "18000 0\n",
      "19000 0\n",
      "20000 0\n",
      "21000 0\n",
      "22000 0\n",
      "23000 0\n",
      "24000 0\n",
      "25000 0\n",
      "26000 0\n",
      "27000 0\n",
      "28000 0\n",
      "29000 0\n",
      "30000 0\n",
      "31000 71\n",
      "32000 71\n",
      "33000 74\n",
      "34000 79\n",
      "35000 79\n",
      "36000 80\n",
      "37000 80\n",
      "38000 80\n",
      "39000 80\n",
      "40000 80\n",
      "41000 80\n",
      "42000 80\n"
     ]
    }
   ],
   "source": [
    "data_dir = r\"data_2\"\n",
    "mp_hands =  mp.solutions.hands\n",
    "hands_model  = mp_hands.Hands(static_image_mode=True, max_num_hands=2 , min_detection_confidence=0.1)\n",
    "\n",
    "data = []\n",
    "errors = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for class_name in os.listdir(data_dir):\n",
    "    class_dir = os.path.join(data_dir , class_name)\n",
    "\n",
    "\n",
    "    for image_file in os.listdir(class_dir):\n",
    "        count +=1\n",
    "        image_path = os.path.join(class_dir , image_file)\n",
    "        image  =cv2.imread(image_path)\n",
    "        image =  cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image , (128 , 128))\n",
    "        results = hands_model.process(image)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks_1 = results.multi_hand_landmarks[0]\n",
    "            \n",
    "            points1 = torch.tensor([[p.x , p.y , p.z] for p in hand_landmarks_1.landmark])\n",
    "            \n",
    "            if len(results.multi_hand_landmarks) >1:\n",
    "                    hand_landmarks_2 = results.multi_hand_landmarks[1]\n",
    "                    points2 = torch.tensor([[p.x , p.y , p.z] for p in hand_landmarks_2.landmark])\n",
    "            else:\n",
    "                points2=  torch.zeros(21,3)\n",
    "            \n",
    "            points = torch.cat([points1,points2] , dim=0)\n",
    "\n",
    "            data.append((points ,class_name))\n",
    "        else:\n",
    "            errors.append(image_path)\n",
    "        \n",
    "        if count%1000 == 0:\n",
    "            print(count , len(errors))\n",
    "\n",
    "torch.save(data , \"tensordata\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7099e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data =  torch.load(\"tensordata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a489110f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41920, 42, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = os.listdir(data_dir)\n",
    "label_map = {k : v  for v , k in enumerate(labels)}\n",
    "\n",
    "X = torch.stack([t[0] for t in tensor_data])\n",
    "Y = torch.stack([torch.tensor(label_map[t[1]]) for t in tensor_data])\n",
    "X.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ec84013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4281)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1][-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf6b5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset ,DataLoader ,random_split\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X , Y)\n",
    "\n",
    "train_len = int(0.8 * len(dataset))\n",
    "valid_len= len(dataset) - train_len\n",
    "\n",
    "train_dataset , valid_dataset = random_split(dataset , [train_len , valid_len])\n",
    "\n",
    "traning_loader = DataLoader( train_dataset , batch_size= 32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset , batch_size=32 , shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af5c0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx_to_label = {v : k for k , v in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01329ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size =len(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07a17c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Module  \n",
    "\n",
    "class Feather_MLP(Module):\n",
    "    def __init__(self , xyz , hidden_size ,feathers_size ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(xyz , hidden_size , bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size , hidden_size , bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size , feathers_size)\n",
    "\n",
    "        self.leakRL = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x ):\n",
    "        B , N , _ = x.shape\n",
    "\n",
    "        #1 layer\n",
    "        x = self.fc1(x)\n",
    "        x = x.transpose(1,2) # (B,N,C) --> (B,C,N) for BatchNorm\n",
    "        x = self.bn1(x).transpose(1,2)\n",
    "        x = self.leakRL(x)\n",
    "\n",
    "        #2 layer\n",
    "        x = self.fc2(x)\n",
    "        x=self.bn2(x.transpose(1,2)).transpose(1,2)\n",
    "        x = self.leakRL(x)\n",
    "\n",
    "        #out\n",
    "        x = self.out(x)\n",
    "        x= self.leakRL(x)\n",
    "\n",
    "        return x # (B,  N , feather_size)\n",
    "    \n",
    "class Sign_language_PointNet(Module):\n",
    "    def __init__(self , mlp , hidden_size, out_size ):\n",
    "        super().__init__()\n",
    "        self.feather_mlp = mlp\n",
    "        feather_size = mlp.out.out_features\n",
    "        \n",
    "        self.fc1 = nn.Linear(feather_size ,hidden_size )\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size , hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size ,out_size)\n",
    "\n",
    "        self.leakRL = nn.LeakyReLU(0.1)    \n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.feather_mlp(x)\n",
    "\n",
    "        x = torch.max(x , dim=1)[0] #(B,feathers)\n",
    "        #layer  1 \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.leakRL(x)\n",
    "\n",
    "        #layer 2 \n",
    "        x = self.fc2(x)\n",
    "        x =self.bn2(x)\n",
    "        x = self.leakRL(x)\n",
    "        \n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "    def detect(self):\n",
    "        #TODO cv to mediapipe\n",
    "        mp_hands = mp.solutions.hands\n",
    "        hands_model = mp_hands.Hands(static_image_mode=False, max_num_hands=2 , min_detection_confidence=0.8)\n",
    "\n",
    "        cap = cv2.VideoCapture(0 , cv2.CAP_DSHOW)\n",
    "\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            \n",
    "            frame = cv2.flip(frame,1)\n",
    "            frame_rgb= cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n",
    "            frame_resize = cv2.resize(frame_rgb , (128,128))\n",
    "            results  = hands_model.process(frame_resize)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                hand_landmarks_1 = results.multi_hand_landmarks[0]\n",
    "            \n",
    "                points1 = torch.tensor([[p.x , p.y , p.z] for p in hand_landmarks_1.landmark])\n",
    "                \n",
    "                if len(results.multi_hand_landmarks) >1:\n",
    "                        hand_landmarks_2 = results.multi_hand_landmarks[1]\n",
    "                        points2 = torch.tensor([[p.x , p.y , p.z] for p in hand_landmarks_2.landmark])\n",
    "                else:\n",
    "                    points2=  torch.zeros(21,3)\n",
    "                \n",
    "                points = torch.cat([points1,points2] , dim=0)\n",
    "\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    points = points.unsqueeze(0)\n",
    "                    logits = self(points)\n",
    "                    preds = torch.softmax(logits , dim=1)\n",
    "                    conf , idx = torch.max(preds, dim=1)\n",
    "                \n",
    "                label = idx_to_label[idx.item()]\n",
    "                \n",
    "                pred_text = f\"It is {label} with {conf.item():.2f} accuracy\"\n",
    "\n",
    "\n",
    "                cv2.putText(frame ,pred_text , (10,40) , cv2.FONT_HERSHEY_SIMPLEX,1 , (0,255,0),2 )    \n",
    "\n",
    "            cv2.imshow(\"kamerka\" , frame)\n",
    "            \n",
    "            if (cv2.waitKey(1) & 0xFF) in (ord('q') ,27):\n",
    "                break\n",
    "            \n",
    "    \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43afcb4",
   "metadata": {},
   "source": [
    "test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "078e2d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6945416927337646\n",
      "3.5553480614894135\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "test_run_mlp = Feather_MLP(3,64,64)\n",
    "test_run_model = Sign_language_PointNet(test_run_mlp , 128,out_size)\n",
    "\n",
    "c = nn.CrossEntropyLoss()\n",
    "for x , y in traning_loader:\n",
    "    \n",
    "    output= test_run_model(x)\n",
    "    loss = c(output,y)\n",
    "    print(loss.item())\n",
    "    break\n",
    "\n",
    "print(log(out_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44c6cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 15:15:05,600] A new study created in memory with name: no-name-2d58e076-7b43-4f7f-8a6f-1583e72339ab\n",
      "[I 2025-10-23 15:15:30,119] Trial 0 finished with value: 0.023767300138023743 and parameters: {'mlp_hidden_size': 454, 'feather_size': 86, 'hidden_size': 311, 'lr': 0.0024847983862204973}. Best is trial 0 with value: 0.023767300138023743.\n",
      "[I 2025-10-23 15:15:54,486] Trial 1 finished with value: 0.00019302407394248622 and parameters: {'mlp_hidden_size': 212, 'feather_size': 119, 'hidden_size': 295, 'lr': 0.00044291981425417435}. Best is trial 1 with value: 0.00019302407394248622.\n",
      "[I 2025-10-23 15:16:19,038] Trial 2 finished with value: 0.0012153931309526586 and parameters: {'mlp_hidden_size': 426, 'feather_size': 498, 'hidden_size': 409, 'lr': 0.00030379314886330445}. Best is trial 1 with value: 0.00019302407394248622.\n",
      "[I 2025-10-23 15:16:42,701] Trial 3 finished with value: 0.0003463634544578531 and parameters: {'mlp_hidden_size': 461, 'feather_size': 458, 'hidden_size': 214, 'lr': 0.002726519737642177}. Best is trial 1 with value: 0.00019302407394248622.\n",
      "[I 2025-10-23 15:17:06,384] Trial 4 finished with value: 0.0012099272779638897 and parameters: {'mlp_hidden_size': 364, 'feather_size': 270, 'hidden_size': 399, 'lr': 0.004080705462663547}. Best is trial 1 with value: 0.00019302407394248622.\n",
      "[I 2025-10-23 15:17:29,810] Trial 5 finished with value: 0.02889401760656964 and parameters: {'mlp_hidden_size': 502, 'feather_size': 103, 'hidden_size': 263, 'lr': 0.0025272489550715385}. Best is trial 1 with value: 0.00019302407394248622.\n",
      "[I 2025-10-23 15:17:59,425] Trial 6 finished with value: 0.0028823259726717156 and parameters: {'mlp_hidden_size': 116, 'feather_size': 491, 'hidden_size': 219, 'lr': 0.005828267763699903}. Best is trial 1 with value: 0.00019302407394248622.\n",
      "[I 2025-10-23 15:18:23,006] Trial 7 finished with value: 0.0006958911287485452 and parameters: {'mlp_hidden_size': 101, 'feather_size': 60, 'hidden_size': 258, 'lr': 0.0006048314619022621}. Best is trial 1 with value: 0.00019302407394248622.\n",
      "[I 2025-10-23 15:18:46,608] Trial 8 finished with value: 0.02666905146612713 and parameters: {'mlp_hidden_size': 492, 'feather_size': 343, 'hidden_size': 403, 'lr': 0.001075956439891417}. Best is trial 1 with value: 0.00019302407394248622.\n",
      "[I 2025-10-23 15:19:10,373] Trial 9 finished with value: 0.019141835721524232 and parameters: {'mlp_hidden_size': 78, 'feather_size': 99, 'hidden_size': 326, 'lr': 0.0001091319264195342}. Best is trial 1 with value: 0.00019302407394248622.\n",
      "[I 2025-10-23 15:19:33,832] Trial 10 finished with value: 0.008510561350676167 and parameters: {'mlp_hidden_size': 224, 'feather_size': 206, 'hidden_size': 33, 'lr': 0.00016465904608869926}. Best is trial 1 with value: 0.00019302407394248622.\n",
      "[I 2025-10-23 15:19:57,178] Trial 11 finished with value: 0.00016092590237022907 and parameters: {'mlp_hidden_size': 267, 'feather_size': 411, 'hidden_size': 144, 'lr': 0.0010548987175552015}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:20:20,893] Trial 12 finished with value: 0.002023893168634528 and parameters: {'mlp_hidden_size': 244, 'feather_size': 344, 'hidden_size': 133, 'lr': 0.0007964437096213129}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:20:44,431] Trial 13 finished with value: 0.0009699804844272071 and parameters: {'mlp_hidden_size': 311, 'feather_size': 185, 'hidden_size': 506, 'lr': 0.0003959556273901337}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:21:08,112] Trial 14 finished with value: 0.00045221799279042483 and parameters: {'mlp_hidden_size': 164, 'feather_size': 404, 'hidden_size': 132, 'lr': 0.0012845220370575861}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:21:31,575] Trial 15 finished with value: 0.0009292147548272513 and parameters: {'mlp_hidden_size': 181, 'feather_size': 245, 'hidden_size': 147, 'lr': 0.0003049975536980095}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:21:54,656] Trial 16 finished with value: 0.008208098673340821 and parameters: {'mlp_hidden_size': 310, 'feather_size': 157, 'hidden_size': 50, 'lr': 0.009546092159548235}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:22:18,111] Trial 17 finished with value: 0.00977214506487602 and parameters: {'mlp_hidden_size': 361, 'feather_size': 333, 'hidden_size': 182, 'lr': 0.0014672818015458206}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:22:41,227] Trial 18 finished with value: 0.001278658890205823 and parameters: {'mlp_hidden_size': 34, 'feather_size': 421, 'hidden_size': 87, 'lr': 0.000541270234822431}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:23:04,642] Trial 19 finished with value: 0.06862505160641806 and parameters: {'mlp_hidden_size': 187, 'feather_size': 34, 'hidden_size': 338, 'lr': 0.00016390413645820192}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:23:27,932] Trial 20 finished with value: 0.002215382430020896 and parameters: {'mlp_hidden_size': 289, 'feather_size': 150, 'hidden_size': 189, 'lr': 0.001519997343554326}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:23:51,481] Trial 21 finished with value: 0.012269102151761057 and parameters: {'mlp_hidden_size': 360, 'feather_size': 452, 'hidden_size': 219, 'lr': 0.002236269978447752}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:24:15,309] Trial 22 finished with value: 0.0021647384510305604 and parameters: {'mlp_hidden_size': 245, 'feather_size': 396, 'hidden_size': 229, 'lr': 0.003711957104143406}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:24:38,955] Trial 23 finished with value: 0.005989114649782172 and parameters: {'mlp_hidden_size': 420, 'feather_size': 442, 'hidden_size': 294, 'lr': 0.0007857352201167305}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:25:02,392] Trial 24 finished with value: 0.10843300742660614 and parameters: {'mlp_hidden_size': 205, 'feather_size': 304, 'hidden_size': 89, 'lr': 0.0018822575895681218}. Best is trial 11 with value: 0.00016092590237022907.\n",
      "[I 2025-10-23 15:25:25,801] Trial 25 finished with value: 0.00013065800526731902 and parameters: {'mlp_hidden_size': 138, 'feather_size': 468, 'hidden_size': 357, 'lr': 0.00046375180224555117}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:25:49,344] Trial 26 finished with value: 0.0005517917558551418 and parameters: {'mlp_hidden_size': 136, 'feather_size': 358, 'hidden_size': 354, 'lr': 0.00043542286455630305}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:26:12,834] Trial 27 finished with value: 0.0002571803480898828 and parameters: {'mlp_hidden_size': 153, 'feather_size': 241, 'hidden_size': 463, 'lr': 0.00031675430331957316}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:26:36,677] Trial 28 finished with value: 0.0004351420375106493 and parameters: {'mlp_hidden_size': 273, 'feather_size': 377, 'hidden_size': 382, 'lr': 0.00019852697657475973}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:26:59,979] Trial 29 finished with value: 0.0018775849473218452 and parameters: {'mlp_hidden_size': 56, 'feather_size': 510, 'hidden_size': 304, 'lr': 0.0008009717023852431}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:27:23,750] Trial 30 finished with value: 0.0003338377508543652 and parameters: {'mlp_hidden_size': 214, 'feather_size': 289, 'hidden_size': 443, 'lr': 0.00023237838010372098}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:27:47,407] Trial 31 finished with value: 0.000978549058647387 and parameters: {'mlp_hidden_size': 144, 'feather_size': 228, 'hidden_size': 482, 'lr': 0.0005331078295929078}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:28:11,004] Trial 32 finished with value: 0.0006695428719953589 and parameters: {'mlp_hidden_size': 170, 'feather_size': 148, 'hidden_size': 444, 'lr': 0.00031640314765346957}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:28:34,692] Trial 33 finished with value: 0.2857290635179579 and parameters: {'mlp_hidden_size': 108, 'feather_size': 480, 'hidden_size': 367, 'lr': 0.00038154978832020817}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:28:58,499] Trial 34 finished with value: 0.0004705390600496032 and parameters: {'mlp_hidden_size': 249, 'feather_size': 311, 'hidden_size': 433, 'lr': 0.00024485617529251043}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:29:21,878] Trial 35 finished with value: 0.00013209894774013437 and parameters: {'mlp_hidden_size': 143, 'feather_size': 431, 'hidden_size': 280, 'lr': 0.0006474026623455515}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:29:45,689] Trial 36 finished with value: 0.0021725074076296737 and parameters: {'mlp_hidden_size': 83, 'feather_size': 477, 'hidden_size': 289, 'lr': 0.001004733082876994}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:30:09,352] Trial 37 finished with value: 0.00019098632586807794 and parameters: {'mlp_hidden_size': 310, 'feather_size': 436, 'hidden_size': 257, 'lr': 0.0006927228974370076}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:30:33,126] Trial 38 finished with value: 0.0004767304195091906 and parameters: {'mlp_hidden_size': 329, 'feather_size': 434, 'hidden_size': 172, 'lr': 0.0006590699998577391}. Best is trial 25 with value: 0.00013065800526731902.\n",
      "[I 2025-10-23 15:30:56,801] Trial 39 finished with value: 0.10548094766150506 and parameters: {'mlp_hidden_size': 397, 'feather_size': 468, 'hidden_size': 255, 'lr': 0.0010156716613667271}. Best is trial 25 with value: 0.00013065800526731902.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def train_eval(model,lr, train_data, valid_data , epoch = 5, device = \"cuda\"):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters() , lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    train_losses, valid_losses = [] , []\n",
    "    \n",
    "    \n",
    "    for _ in range(epoch):\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for x , y in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            x , y = x.to(device) , y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output , y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * y.size(0)\n",
    "        train_loss = running_loss/ len(train_data.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for x , y in valid_data:\n",
    "                x , y = x.to(device) , y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output , y)\n",
    "                \n",
    "                running_loss += loss.item() * y.size(0)\n",
    "            valid_loss = running_loss/ len(valid_data.dataset)\n",
    "            valid_losses.append(valid_loss)\n",
    "    \n",
    "    return train_losses , valid_losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trail):\n",
    "\n",
    "    mlp_hidden_size = trail.suggest_int(\"mlp_hidden_size\" ,32,512)\n",
    "    feather_size = trail.suggest_int(\"feather_size\" ,32,512)\n",
    "    hidden_size = trail.suggest_int(\"hidden_size\" , 32, 512)\n",
    "    lr = trail.suggest_float(\"lr\" , 1e-4, 1e-2 , log=True)  \n",
    "\n",
    "\n",
    "    mlp = Feather_MLP(xyz =3,hidden_size= mlp_hidden_size , feathers_size= feather_size )\n",
    "    model = Sign_language_PointNet(mlp = mlp , hidden_size= hidden_size, out_size = out_size)\n",
    "    _ , valid_loss = train_eval(model , lr , traning_loader , valid_loader)\n",
    "    \n",
    "\n",
    "    return valid_loss[-1]\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a46ba69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlp_hidden_size': 138,\n",
       " 'feather_size': 468,\n",
       " 'hidden_size': 357,\n",
       " 'lr': 0.00046375180224555117}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e756c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"best_params.json\" ,\"w\") as f:\n",
    "    json.dump(study.best_params , f , indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1adf2b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38563"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in test_run_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3549b1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.07718609903820431,\n",
       "  0.00876847311639083,\n",
       "  0.005530294407449462,\n",
       "  0.003924525031327674,\n",
       "  0.001883158174685044],\n",
       " [0.0018027706065291596,\n",
       "  0.007984826500898134,\n",
       "  0.00020499107760756246,\n",
       "  0.001334009736728364,\n",
       "  0.008502680596692012])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = study.best_params \n",
    "hidden_size_mlp = p.get(\"mlp_hidden_size\") \n",
    "feather_size = p.get(\"feather_size\") \n",
    "hidden_size = p.get(\"hidden_size\") \n",
    "lr = p.get(\"lr\") \n",
    "\n",
    "feather_model = Feather_MLP(xyz=3 , hidden_size=hidden_size_mlp, feathers_size=feather_size )\n",
    "model = Sign_language_PointNet(mlp = feather_model , hidden_size=hidden_size ,out_size=out_size )\n",
    "\n",
    "train_eval(model,lr,traning_loader , valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7aa2b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict() , \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f01de4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feather_model = Feather_MLP(xyz=3 , hidden_size=hidden_size_mlp, feathers_size=feather_size)\n",
    "model = Sign_language_PointNet(mlp = feather_model , hidden_size=hidden_size ,out_size=out_size )\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "model.detect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
